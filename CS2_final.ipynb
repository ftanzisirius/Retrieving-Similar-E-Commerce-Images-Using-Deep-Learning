{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "CS2_final.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "Sy5sK0G6RQsQ"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy5sK0G6RQsQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0gO2pphhwwZg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Input, MaxPool2D, GlobalAveragePooling2D, Lambda, Conv2D, concatenate, ZeroPadding2D, Layer, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "import tensorflow_addons as tfa"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKAChU-Hhqe_",
    "outputId": "fe286c2e-c864-4629-ac3e-2bdfbdcdeefe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xSy7rLtBU1jB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#!cp '/content/drive/My Drive/MILDNet/tops_cropped.zip' '/content/'"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjhI8zviVGyi",
    "outputId": "8e05a8ad-3145-4faa-b589-a14cc023cb58",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#!unzip './content/tops_cropped.zip'"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Alg1WWi4RXCd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYuJbH3aRZ2A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2> Case Study Details </h2>\n",
    "Ref Paper Link: https://arxiv.org/pdf/1901.03546v1.pdf\n",
    "\n",
    "In this we are trying to learn the embedding of images in order to learn the visual similarity. Finding products that looks similar to a particular product is an important feature for a modern e-commerce platform. If this information is utilized correctly, can boost up user experience and purchase conversion. So, with this case study we are trying to implement a research work which uses siamense network at core to retrieve similar E-commerce images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2FHmppvRZ5J",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2> Dataset Used </h2>\n",
    "As per the paper they have mentioned 4 different data sources using which they arrived at the final triplets for training, testing and validation purpose.\n",
    "They have already shared the link from where we can actually download the csv having information for of these triplets. https://console.cloud.google.com/storage/browser/fynd-open-source/research/MILDNet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmIfjR6NRZ9M",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> Implementation Steps </h3>\n",
    "\n",
    "We are going to implement this work in TF2 and we'll writing custom training loops to implement this work. At the core of any NN implementation we need to identify following blocks:\n",
    "1. Creating data pipeline.\n",
    "2. Defining the model architecture.\n",
    "3. Identifying the loss function used for computing the model loss.\n",
    "4. Choosing optimizer used to update model weights.\n",
    "5. Step function that encapsulates the forward & backward pass of the network.\n",
    "\n",
    "Below mentioned implementation functions are written to encapsulate all these steps and make use of TF2 custom training functionality to implement the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT3qLagYiW4Z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> Improvements and other updates from previous Submission </h3>\n",
    "\n",
    "1. **Overfitting:** In last submission we were actually overfitting, reason for that was a bug in the code itself, we were actually passing the same batch of images for all the steps, so, we were actually training on same 32 images and thus overfitting.\n",
    "2. **parser:** parser function has been modified as per your comments, now we don't iterate inside that, rather than dataset.map takes care of this functionality.\n",
    "3. **input_pipeline:** This has also been changed, after reading file details from the tensor, we are doing a shuffle, then map function and then the batch functionality, shuffle is done at epoch level.\n",
    "4. **Loss Function Implementation:** We have tfa.losses.contrastive_loss added to our code, just to let you know that if we pass a tensor of 32 to our loss, the resultant would again be a tensor of shape 32, so to find the final loss anyhow we may need to iterate and calculate the same way we did in our function, so have not made much changes to that function as a whole, instead have just added tfa.losses implementation instead of our code, although result from both these functions were same.\n",
    "5. **flow function implementation:** Few changes have been made to how we get loss and accuracy value per epoch, in our last implementation, we were printing value based on the last step. but now during training we keep storing loss and accuracy for each batch and final score we give by taking average of these values. For validation once all train batches are done, we then take a batch size of 128 and iterate through all the images in val dataset and finally give the average value of loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUhb3yyeRaAc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDZq8M68nurF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> Data Pipeline Methods</h3>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MPayHznJhjp2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def get_image_path(path):\n",
    "  '''\n",
    "  This function takes the path of the csv file as input and returns a list where each element\n",
    "  of the list contains a triplet with the path where the actual image is present.\n",
    "  path(input): path to the csv file for train/test/cv\n",
    "  img_path: path where we have kept our images\n",
    "  data_path_lst(output): final list which contains triplet details for input pipeline\n",
    "  '''\n",
    "  data_path_lst = []\n",
    "  \n",
    "  global img_path \n",
    "  with open(path) as csv_file:\n",
    "    data = csv.reader(csv_file, delimiter = ',')\n",
    "    for row in data:\n",
    "      data_path_lst.append([img_path + row[0], img_path + row[1], img_path + row[2]])\n",
    "\n",
    "  return data_path_lst"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HIr9GXk08ob",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**get_image_path:** Given a path of the csv file, this function opens the file and reads the content in that file using csv.reader, then it iterates over all the rows present in the file and for each row it adds image path before the image name and create a final list which is returned and will be used for reading during input pipeline...."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "acT7LHjsqXze",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def input_parser(input_tensor):\n",
    "  '''\n",
    "  This function is used as part of data pipeline which reads image path as part of data.Dataset\n",
    "  and generate the final array for the image using tf.image.decode_jpeg. 3 operations are performed\n",
    "  in this function. For an image, first we read the path from the input tensor using tf.io.read_file, \n",
    "  then we use tf.image.decode_jpeg to decode the image and get a tesnor for the image and finally we\n",
    "  use tf.image.resize to resize the image as per our architecture.\n",
    "  input_tensor(input): tensor having image path from where images will be loaded.\n",
    "  batch_size(input): Number of bacth\n",
    "  img1(output): containing final tensor for query images\n",
    "  img2(output): containing final tensor for positive images\n",
    "  img3(output): containing final tensor for negative images\n",
    "  '''\n",
    "  img1 = tf.image.resize(tf.image.decode_jpeg(tf.io.read_file(input_tensor[0]), channels=3), [224, 224])\n",
    "  img2 = tf.image.resize(tf.image.decode_jpeg(tf.io.read_file(input_tensor[1]), channels=3), [224, 224])\n",
    "  img3 = tf.image.resize(tf.image.decode_jpeg(tf.io.read_file(input_tensor[2]), channels=3), [224, 224])\n",
    "\n",
    "  return img1, img2, img3"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twoLXzRj1luG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**input_parser:** This function is used by dataset.map to get the feature tensor for each of the image. It starts by reading the file path using read_file, once this is done it extracts feature vector from an image using tf.image.decode_jpeg and finally it reshapes the final tensor per image to desired format which will be feed into the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "indH-ezljnzO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def input_pipeline(data_path, shuffle_size, batch_size):\n",
    "  '''\n",
    "  This function takes in data_path and the batch_size and return final tensors\n",
    "  representing each of the image.\n",
    "  data_path(input): final list which contains triplet details for input pipeline\n",
    "  batch_size(input): number of records for each step\n",
    "  dataset(output): final dataset object containig tensor of images with a fixed batch size\n",
    "  '''\n",
    "  with tf.device('/cpu:0'):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((np.array((data_path))))\n",
    "    dataset = dataset.shuffle(shuffle_size)\n",
    "    dataset = dataset.map(input_parser, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size).repeat()\n",
    "    dataset = dataset.prefetch(1)\n",
    "\n",
    "  return dataset"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzwctH_L4P05",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**input_pipeline:** This function uses tf.data to define the input pipeline. In the above implementation we read the input file name from an array which contains details of input file path for each image. Then we shuffle the content readed. Then we map use input_parser to get feature vector for each image and then that final tensor as dataset object is returned which will be used during modelling. Finally, we take a batch of images which are called using an iterator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgCj8z06ZsGq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> Model Architecture </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTznGAvh-r1S",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![](https://drive.google.com/uc?export=view&id=112oYi4BCAugrwxLfls7-58KwaJ5wOvI3)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Nkpi_Swrhgop",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def model_architecture():\n",
    "  '''\n",
    "  Ref: https://github.com/gofynd/mildnet/blob/master/trainer/model.py\n",
    "  This function contains model architecture which will be used to generate final embedding for each image.\n",
    "  model(output): Final model object\n",
    "  '''\n",
    "  pre_trained_model = VGG19(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
    "  convnet = GlobalAveragePooling2D()(pre_trained_model.output)\n",
    "  convnet = Dense(4096, activation = 'relu')(convnet)\n",
    "  convnet = Dropout(0.5)(convnet)\n",
    "  convnet = Dense(4096, activation = 'relu')(convnet)\n",
    "  convnet = Dropout(0.5)(convnet)\n",
    "  convnet = Lambda(lambda x : K.l2_normalize(x, axis = 1))(convnet)\n",
    "\n",
    "  s1_inp = Input(shape = (224, 224, 3))\n",
    "  s1 = MaxPool2D(pool_size = (4, 4), strides = (4, 4), padding = 'valid')(s1_inp)\n",
    "  s1 = ZeroPadding2D(padding = (4, 4), data_format = None)(s1)\n",
    "  s1 = Conv2D(96, kernel_size = (8, 8), strides = (4, 4), padding = 'valid')(s1)\n",
    "  s1 = ZeroPadding2D(padding = (2, 2), data_format = None)(s1)\n",
    "  s1 = MaxPool2D(pool_size = (7, 7), strides = (4, 4), padding = 'valid')(s1)\n",
    "  s1 = Flatten()(s1)\n",
    "\n",
    "  s2_inp = Input(shape = (224, 224, 3))\n",
    "  s2 = MaxPool2D(pool_size = (8, 8), strides = (8, 8), padding = 'valid')(s2_inp)\n",
    "  s2 = ZeroPadding2D(padding = (4, 4), data_format = None)(s2)\n",
    "  s2 = Conv2D(96, kernel_size = (8, 8), strides = (4, 4), padding = 'valid')(s2)\n",
    "  s2 = ZeroPadding2D(padding = (1, 1), data_format = None)(s2)\n",
    "  s2 = MaxPool2D(pool_size = (3, 3), strides = (2, 2), padding = 'valid')(s2)\n",
    "  s2 = Flatten()(s2)\n",
    "\n",
    "  merge = concatenate([s1, s2])\n",
    "  merge = Lambda(lambda x : K.l2_normalize(x, axis = 1))(merge)\n",
    "  merge = concatenate([merge, convnet], axis = 1)\n",
    "  embedding = Dense(4096)(merge)\n",
    "  embedding = Lambda(lambda x : K.l2_normalize(x, axis = 1))(embedding)\n",
    "\n",
    "  model = tf.keras.models.Model(inputs = [s1_inp, s2_inp, pre_trained_model.input], outputs = embedding)\n",
    "  return model"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhWvyH9NAlBh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**model_architecture:** Above function implements the actual model architecture, an input image is being passed to 3 different CNN architectures.\n",
    "The first one is similar to VGG19, is used to encode strong invariance and capture the semantics present in an image. The other two CNN's use a shallower network architecture to capture down-sampled images. Due to the shallower architecture, these CNNs' have less invariance and are used to capture similar aspects like shapes, pattern, and color which makes the visual appearence of an image.  Thus employing three different convolution neural networks instead of a single CNN and making them share lower level layers, makes each CNN independent of the other two. At last, the embeddings from the three convolutional neural networks are normalized and combined with a 4096-dimensional linear embedding layer which encodes and represents an input image as a 4096-dimensional\n",
    "vector. In order to prevent overfitting, L2 normalization is done.\n",
    "Ref: https://arxiv.org/pdf/1901.03546v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKrkcbc_Z4z3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> Loss Function Implementation </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbemeKqN-YOL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MkHqlrkCqCvY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def contrastive_loss_function(q_emd, p_emd, n_emd, batch_size):\n",
    "  '''\n",
    "  Ref: https://github.com/gofynd/mildnet/blob/master/trainer/loss.py\n",
    "  This function takes embedding generated by model for each of the image\n",
    "  part of the triplet and return the loss value for the batch.\n",
    "  q_emd(input): embedding generated by model for query image(tensor of size [batch, 4096])\n",
    "  p_emd(input): embedding generated by model for query image(tensor of size [batch, 4096])\n",
    "  n_emd(input): embedding generated by model for query image(tensor of size [batch, 4096])\n",
    "  batch_size(input): batch size for each step\n",
    "  loss(output): Final loss for a batch\n",
    "  '''\n",
    "  def _contrastive_loss(y_true, y_pred):\n",
    "    return tfa.losses.contrastive_loss(y_true, y_pred)\n",
    "\n",
    "  loss = tf.convert_to_tensor(0,dtype=tf.float32)\n",
    "  g = tf.constant(1.0, shape=[1], dtype=tf.float32)\n",
    "  h = tf.constant(0.0, shape=[1], dtype=tf.float32)\n",
    "\n",
    "  for obs_num in range(batch_size):\n",
    "    dist_query_pos = tf.sqrt(tf.reduce_sum((q_emd[obs_num] - p_emd[obs_num])**2))\n",
    "    dist_query_neg = tf.sqrt(tf.reduce_sum((q_emd[obs_num] - n_emd[obs_num])**2))\n",
    "    loss_query_pos = _contrastive_loss(g, dist_query_pos)\n",
    "    loss_query_neg = _contrastive_loss(h, dist_query_neg)\n",
    "    loss = loss + loss_query_pos + loss_query_neg\n",
    "\n",
    "  loss = loss/(batch_size*2)\n",
    "  zero = tf.constant(0.0, shape=[1], dtype=tf.float32)\n",
    "  return tf.maximum(loss, zero)"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "It0ozVxSELpT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**contrastive_loss_function:** This is the loss implementation of the contrastive loss function. The mathmatical objective of this loss is mentioned before the code cell. This function calcultes loss for a batch of images. For one bacth it startes by calculating loss value for each of the pair. Once we have that value, that value is added to the final loss and then loss is normalized by diving it by the 2 time the batch_size.\n",
    "In the loss equation, label Y = 1 is assigned to dissimilar or negative image pairs whereas Y = 0 is alloted to similar or positive image pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wo2csQNsZ8PA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> Accuracy function </h3>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tTGnLDA2OEG1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def accuracy(q_emd, p_emd, n_emd, batch_size):\n",
    "  '''\n",
    "  Ref: https://github.com/gofynd/mildnet/blob/master/trainer/accuracy.py\n",
    "  This function takes in embedding and return the accuracy value for the batch\n",
    "  q_emd(input): embedding generated by model for query image(tensor of size [batch, 4096])\n",
    "  p_emd(input): embedding generated by model for query image(tensor of size [batch, 4096])\n",
    "  n_emd(input): embedding generated by model for query image(tensor of size [batch, 4096])\n",
    "  batch_size(input): batch size for each step\n",
    "  accuracy(output): Final accuracy value for a batch\n",
    "  '''\n",
    "  accuracy = 0\n",
    "  for obs_num in range(batch_size):\n",
    "    dist_query_pos = tf.sqrt(tf.reduce_sum((q_emd[obs_num] - p_emd[obs_num])**2))\n",
    "    dist_query_neg = tf.sqrt(tf.reduce_sum((q_emd[obs_num] - n_emd[obs_num])**2))\n",
    "    accuracy += tf.cond(dist_query_neg > dist_query_pos, lambda : 1, lambda : 0)\n",
    "\n",
    "  return (accuracy * 100) / batch_size"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_uyMoYbFGgr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**accuracy:** This function calculates the accuracy for one batch. It takes the embeding tensors for each of the image, batch_size and iterator through the batch to give a value of 1 if neg_dist > pos_dist or 0 otherwise. It finally returns the final accuracy for a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luPaVcg1aCvn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> Train Step Implementation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6JgmOopQMeZT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "@tf.function\n",
    "def train_step(data, batch_size):\n",
    "  '''\n",
    "  This is the custom function which will be responsible for trainig based\n",
    "  on the training dataset, here we first calculate the embedding for our triplet.\n",
    "  Then, we'll find the loss and accuracy value using the embedded tensors.\n",
    "  Finally, we use loss value and gradient tape to calculate the gradients and \n",
    "  optimizer to update weights. All of these operations are done for one step and\n",
    "  we keep on updating weights untill convergence.\n",
    "  data(input): input tensor\n",
    "  loss_value(output): loss value for the batch before updating weights\n",
    "  accuracy_value(output): accuracy value for the batch before updating weights\n",
    "  '''\n",
    "  with tf.GradientTape() as tape:\n",
    "    q_emd = model((data[0], data[0], data[0]), training = True)\n",
    "    p_emd = model((data[1], data[1], data[1]), training = True)\n",
    "    n_emd = model((data[2], data[2], data[2]), training = True)\n",
    "\n",
    "    loss_value = contrastive_loss_function(q_emd, p_emd, n_emd, batch_size)\n",
    "    accuracy_value = accuracy(q_emd, p_emd, n_emd, batch_size)\n",
    "\n",
    "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "  return loss_value, accuracy_value"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDEfpcCa5F4o",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**train_step:** This is where we use gradient tape to perform actual training. In our case since we are dealing with triplets, for one batch we first find the embedding for each of the image in that batch such that we get embedding for query, positive and negative image. This embedding is generated using model architecture that was defined. Once we have this, we will then calculate loss and accuracy for that batch and then based on the loss retured finally we calculate gradients using the gradient tape. Once we have these gradients with us we use optimizer object to update weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn2qrhrG6KAh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**tf.function decorator is used to cause tensorflow autograph working and accelerate execution for those operation inside it. When we call a @tf.function decorator the first time, tensorflow will first convert it into a graph, then execute it, after that, when we call the function again, it'll just execute the graph.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GivMsz_aIMm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> Validation step implementation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uZhCtJsVuQHW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "@tf.function\n",
    "def val_step(data, batch_size):\n",
    "  '''\n",
    "  This function is the step function which finds the loss value and the accuracy\n",
    "  on a bacth of data for each step in an epoch on validation data. No training is\n",
    "  done within this bock, instead we just find the embedding and based on what model\n",
    "  has learned in the train phase we get the final loss and accuracy on validation data\n",
    "  data(input): input tensor\n",
    "  loss_value(output): loss value for the batch before updating weights\n",
    "  accuracy_value(output): accuracy value for the batch before updating weights\n",
    "  '''\n",
    "  q_emd = model((data[0], data[0], data[0]), training = False)\n",
    "  p_emd = model((data[1], data[1], data[1]), training = False)\n",
    "  n_emd = model((data[2], data[2], data[2]), training = False)\n",
    "\n",
    "  loss_value = contrastive_loss_function(q_emd, p_emd, n_emd, batch_size)\n",
    "  accuracy_value = accuracy(q_emd, p_emd, n_emd, batch_size)\n",
    "\n",
    "  return loss_value, accuracy_value"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCQbth6N6y8C",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**val_step:** This function is responsible for giving us validation loss and accuracy on validation dataset per step for an epoch. This function uses the model(trained with updated weights) to generate embedding and based on those genearted embedding we calculate the loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "226Bt_wWsAWm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> Final Flow Implemetation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qD9nue5NsEy9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def flow(epochs, batch_size):\n",
    "  #define path from where data will be read\n",
    "  #train_csv_file = '/content/drive/My Drive/MILDNet/tops_train.csv'\n",
    "  train_csv_file = './content/tops_train_shuffle.csv'\n",
    "  #cv_csv_file = '/content/drive/My Drive/MILDNet/tops_val.csv'\n",
    "  cv_csv_file = './content/tops_val_full.csv'\n",
    "  val_batch_size = 128\n",
    "\n",
    "  #calling get_image_path to get a list of triplets with actual path\n",
    "  train_data_path = get_image_path(train_csv_file)\n",
    "  validation_data_path = get_image_path(cv_csv_file)\n",
    "  train_loss = tf.metrics.Mean(name=\"train_loss\")\n",
    "  train_accuracy = tf.metrics.Mean(name=\"train_acc\")\n",
    "\n",
    "  #Starting the actual flow from where we will start training using our custom function, enabling GPU for training\n",
    "  with tf.device('/gpu:0'):\n",
    "    for epoch in range(epochs):\n",
    "      epoch_start_time = time.time()\n",
    "      print('*'*100)\n",
    "      print('Epoch: {}'.format(epoch + 1))\n",
    "      train_dataset = input_pipeline(train_data_path, len(train_data_path), batch_size) # calling pipeline method to generate train dataset\n",
    "      train_iterator = iter(train_dataset) # Creating an iterator on the dataset\n",
    "\n",
    "      train_loss.reset_states()\n",
    "      train_accuracy.reset_states()\n",
    "      loss_, acc_ = [], []\n",
    "      for step in range(len(train_data_path) // batch_size):\n",
    "      #for step in range(2):\n",
    "        \n",
    "        train_data = train_iterator.get_next() #Calling a batch of dataset\n",
    "        train_loss_value, train_accuracy_value = train_step(train_data, batch_size) # Calling train step function on the fetched batch\n",
    "        loss_.append(train_loss_value.numpy()[0])\n",
    "        acc_.append(train_accuracy_value)\n",
    "        #print(train_loss)\n",
    "\n",
    "        #Printing loss and accuracy value after every 200 steps in an epoch\n",
    "        if step % 200 == 0:\n",
    "          print('\\t{} train samples seen so far'.format((step + 1) * batch_size)) \n",
    "          print('\\tTraining loss(for one batch) at step {} is {}'.format(step + 1, train_loss_value))\n",
    "          print('\\tTraining accuracy(for one batch) at step {} is {}'.format(step + 1, train_accuracy_value))\n",
    "      \n",
    "      train_loss.update_state(loss_)\n",
    "      train_accuracy.update_state(acc_)\n",
    "\n",
    "      val_accuracy, val_loss = [], []\n",
    "      validation_dataset = input_pipeline(validation_data_path, len(validation_data_path), val_batch_size)\n",
    "      validation_iterator = iter(validation_dataset)\n",
    "      for _ in range(len(validation_data_path) // val_batch_size):\n",
    "        validation_data = validation_iterator.get_next()\n",
    "        validation_loss_value, validation_accuracy_value = val_step(validation_data, val_batch_size)\n",
    "        val_loss.append(validation_loss_value.numpy()[0])\n",
    "        val_accuracy.append(validation_accuracy_value)\n",
    "      \n",
    "      #printing loss and accuracy value after the end of epoch on final batch of that epoch\n",
    "      print('Train loss after epoch {} is: {}'.format(epoch + 1, train_loss.result().numpy()))\n",
    "      print('Validation loss after epoch {} is: {}'.format(epoch + 1, round(np.mean(np.array(val_loss)), 6)))\n",
    "      print('Train Accuracy after epoch {} is: {}'.format(epoch + 1, train_accuracy.result().numpy()))\n",
    "      print('Validation Accuracy after epoch {} is: {}'.format(epoch + 1, round(np.mean(np.array(val_accuracy)), 2)))\n",
    "      print('Time taken to complete epoch {} is: {} mins'.format(epoch + 1, round((time.time() - epoch_start_time) / 60, 2)))\n",
    "      \n",
    "      # write them to the tensorboard\n",
    "      with tf.name_scope(\"per_epoch_params\"):\n",
    "        with wtrain.as_default():\n",
    "          tf.summary.scalar(\"loss\", train_loss.result().numpy(), step=epoch)\n",
    "          tf.summary.scalar(\"acc\", train_accuracy.result().numpy(), step=epoch)\n",
    "          wtrain.flush()\n",
    "\n",
    "        with wval.as_default():\n",
    "          tf.summary.scalar(\"loss\", round(np.mean(np.array(val_loss)), 6), step=epoch)\n",
    "          tf.summary.scalar(\"acc\", round(np.mean(np.array(val_accuracy)), 2), step=epoch)\n",
    "          wval.flush()\n",
    "      model.save_weights('epoch{}model.h5'.format(epoch + 1))\n",
    "      print('Model weights saved!!!!')"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_Dn-7397No0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**flow:** This function call all the other function to perform custom training for our whole architecture. In the above function, we first define the path of the images and the files which will be used to fetch data.\n",
    "Then we start our training loop where we enable the usage of available GPU for faster training. Once this is done, we start by first reading data for a bacth. For doing that we create an iterator object which will give us data in batches. Once this is done, we'll then call the step function to train, we keep on recording the scores we get during each step of training inside an epoch and finally when this is done, we use validation set to calculate the loss and accuracy and print the final one for both train and validation. Model weights are also saved after each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESC1xvAHTUZh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> Executing the main function to train </h3>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ur_rpDC5yJGu",
    "outputId": "eb868820-fb12-4032-82db-1dc00eece07a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "if __name__ == '__main__':\n",
    "  img_path = './content/tops_cropped/'\n",
    "  wtrain = tf.summary.create_file_writer(logdir = 'logs/train')\n",
    "  wval = tf.summary.create_file_writer(logdir = 'logs/val')\n",
    "  model = model_architecture()\n",
    "  optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.001) #defining optimizer\n",
    "  flow(10, 32) # 10 epochs, batch size of 32"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 16:36:39.022976: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-29 16:36:39.023098: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 16:36:39.981646: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-03-29 16:36:45.546987: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t32 train samples seen so far\n",
      "\tTraining loss(for one batch) at step 1 is [0.41115385]\n",
      "\tTraining accuracy(for one batch) at step 1 is 59.375\n",
      "\t6432 train samples seen so far\n",
      "\tTraining loss(for one batch) at step 201 is [0.14294131]\n",
      "\tTraining accuracy(for one batch) at step 201 is 87.5\n",
      "\t12832 train samples seen so far\n",
      "\tTraining loss(for one batch) at step 401 is [0.17357823]\n",
      "\tTraining accuracy(for one batch) at step 401 is 84.375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m model_architecture()\n\u001B[1;32m      6\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mRMSprop(learning_rate \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.001\u001B[39m) \u001B[38;5;66;03m#defining optimizer\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[43mflow\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[14], line 32\u001B[0m, in \u001B[0;36mflow\u001B[0;34m(epochs, batch_size)\u001B[0m\n\u001B[1;32m     30\u001B[0m train_data \u001B[38;5;241m=\u001B[39m train_iterator\u001B[38;5;241m.\u001B[39mget_next() \u001B[38;5;66;03m#Calling a batch of dataset\u001B[39;00m\n\u001B[1;32m     31\u001B[0m train_loss_value, train_accuracy_value \u001B[38;5;241m=\u001B[39m train_step(train_data, batch_size) \u001B[38;5;66;03m# Calling train step function on the fetched batch\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m loss_\u001B[38;5;241m.\u001B[39mappend(\u001B[43mtrain_loss_value\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     33\u001B[0m acc_\u001B[38;5;241m.\u001B[39mappend(train_accuracy_value)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m#print(train_loss)\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m#Printing loss and accuracy value after every 200 steps in an epoch\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/Retrieving-Similar-E-Commerce-Images-Using-Deep-Learning/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1159\u001B[0m, in \u001B[0;36m_EagerTensorBase.numpy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1136\u001B[0m \u001B[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001B[39;00m\n\u001B[1;32m   1137\u001B[0m \n\u001B[1;32m   1138\u001B[0m \u001B[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1156\u001B[0m \u001B[38;5;124;03m    NumPy dtype.\u001B[39;00m\n\u001B[1;32m   1157\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1158\u001B[0m \u001B[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001B[39;00m\n\u001B[0;32m-> 1159\u001B[0m maybe_arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m   1160\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m maybe_arr\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(maybe_arr, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;28;01melse\u001B[39;00m maybe_arr\n",
      "File \u001B[0;32m~/.conda/envs/Retrieving-Similar-E-Commerce-Images-Using-Deep-Learning/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1125\u001B[0m, in \u001B[0;36m_EagerTensorBase._numpy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1123\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_numpy\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1124\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1126\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v3qyK7SRTwxv",
    "outputId": "9a37953d-60ac-4a1e-bf45-fcf4aabaeb5f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "x = PrettyTable()\n",
    "x.field_names = ['Epoch #', 'Train Loss', 'Val Loss', 'Train Accuracy', 'Val Accuracy']\n",
    "\n",
    "x.add_row([1, .1350, .1228, 90.13, 91.97])\n",
    "x.add_row([2, .0845, .1176, 96.09, 93.4])\n",
    "x.add_row([3, .0667, .1158, 97.37, 93.93])\n",
    "x.add_row([4, .0576, .1114, 98.18, 94.19])\n",
    "x.add_row([5, .0515, .1802, 98.64, 92.98])\n",
    "x.add_row([6, .0462, .1371, 98.95, 94.08])\n",
    "x.add_row([7, .0423, .2026, 99.20, 92.25])\n",
    "x.add_row([8, .0385, .1797, 99.36, 92.38])\n",
    "x.add_row([9, .0353, .2226, 99.53, 91.78])\n",
    "x.add_row([10, .0327, .2382, 99.62, 91.22])\n",
    "\n",
    "print(x)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mf5tnalimn6b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see the best results we got was for 4th Epoch, hence that will be used as final model on validation set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63rJj1kXXHpi",
    "outputId": "16bf2716-ef41-49b0-8fd0-98a8fa65bea9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!ls -l epoch*"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ds_1n4VSXZXh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#!cp epoch* '/content/drive/My Drive/weight/'\n",
    "!cp epoch* './content/weight/'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8W0R5YnItij",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Validation Block"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E8bMG_f7yJNx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def val_flow(cv_csv_file):\n",
    "  '''\n",
    "  This function is used to perform validation in 2 steps, first step is by taking\n",
    "  a batch of images and computing loss and accuracy for that bacth, In this \n",
    "  validation exercise we have taken 5 batches which have randomly shuffled images.\n",
    "  2nd validation step is taking just one triplet and finding the embedding and \n",
    "  identifying if model is working correctly or not.\n",
    "  '''\n",
    "  validation_data_path = get_image_path(cv_csv_file)\n",
    "\n",
    "  with tf.device('gpu:0'):\n",
    "    #Performing 5 step validation steps\n",
    "    for cv_step in range(5):\n",
    "      val_batch_size = 128 #Taking a batch size beyond 128 is OOM error\n",
    "\n",
    "      #Generating data for model to predict\n",
    "      validation_dataset = input_pipeline(validation_data_path, len(validation_data_path), val_batch_size)\n",
    "      validation_iterator = iter(validation_dataset)\n",
    "      validation_data = validation_iterator.get_next()\n",
    "     # print(validation_data)\n",
    "     #Calling val_step function to get loss and accuracy value on validation set of 128 images\n",
    "      validation_loss_value, validation_accuracy_value = val_step(validation_data, val_batch_size)\n",
    "      print('Validation loss and accuracy for {} validation step on {} random data points from validation set are: {}, {}'.format(cv_step + 1, val_batch_size, validation_loss_value, validation_accuracy_value))\n",
    "\n",
    "    #Checking model prediction on 5 random data points\n",
    "    for cv_step in range(5):\n",
    "\n",
    "      #Fetching just one image from our pipeline\n",
    "      val_batch_size = 1\n",
    "      validation_dataset = input_pipeline(validation_data_path, len(validation_data_path), val_batch_size)\n",
    "      validation_iterator = iter(validation_dataset)\n",
    "      validation_data = validation_iterator.get_next()\n",
    "\n",
    "      #Getting embedding for the image from the model\n",
    "      q_emd = model((validation_data[0], validation_data[0], validation_data[0]), training = False)\n",
    "      p_emd = model((validation_data[1], validation_data[1], validation_data[1]), training = False)\n",
    "      n_emd = model((validation_data[2], validation_data[2], validation_data[2]), training = False)\n",
    "\n",
    "      #Finding Distance between the images\n",
    "      dist_query_pos = tf.sqrt(tf.reduce_sum((q_emd - p_emd)**2))\n",
    "      dist_query_neg = tf.sqrt(tf.reduce_sum((q_emd - n_emd)**2))\n",
    "\n",
    "      #Generating result\n",
    "      print('For {} sample, distance b/w q & p is {} and distance b/w q & n is {}.... Model performed {}'.format(cv_step + 1, round(dist_query_pos.numpy(), 3), round(dist_query_neg.numpy(), 3), 'correctly' if dist_query_pos < dist_query_neg else 'incorrectly'))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUnIlC6vaYZ-",
    "outputId": "cbf7d2db-19d0-4fb3-9cd0-fb9f3c8075e8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "model = model_architecture() # calling model architecture function\n",
    "#model.load_weights('/content/drive/My Drive/weight/epoch4model.h5') # Loading weights for model saved after 4th epoch\n",
    "model.load_weights('./content/weight/epoch4model.h5')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q2M1DOHTaeNg",
    "outputId": "af67bee5-f4da-4c0b-d0cb-2d99fbf239d1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "img_path = './content/tops_cropped/'\n",
    "#cv_csv_file = '/content/drive/My Drive/MILDNet/tops_val.csv'\n",
    "cv_csv_file = './content/tops_val_full.csv'\n",
    "val_flow(cv_csv_file)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0SqEj9oV3jB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From above we can see that when taken batch of images on an avg we are getting an accuracy of 95%, also, when taken just 1 image for this run model performed correctly all the time for multiple runs....\n"
   ]
  }
 ]
}